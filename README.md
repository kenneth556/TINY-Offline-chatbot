TinyllamaChatbot - Offline AI Chatbot
A fully offline AI chatbot powered by the Tinyllama 1B Instruct v0.2 model using the ctransformers library. Features a clean Tkinter GUI with chat history, configurable settings, and complete offline operation.

Features
Completely Offline: No internet connection required
Local Tinyllama 1B Model: Uses GGUF quantized model for efficient inference
Clean GUI: Tkinter-based interface with scrollable chat history
Configurable: Adjustable temperature, max tokens, GPU layers, and more
Chat History: Automatic logging with timestamps
Cross-Platform: Works on Windows, macOS, and Linux
Requirements
Python 3.8 or higher
At least 4GB RAM (8GB recommended)
The Tinyllama 1B GGUF model file
Installation
Clone or download this project

git clone <repository-url>
cd TinyllamaChatbot
Install dependencies

pip install -r requirements.txt
Download the Tinyllama model

Download Tinyllama-1B-miniguanaco.Q2_K.gguf from Hugging Face
Place it in the model/ directory
The file should be located at: model/Tinyllama-1B-miniguanaco.Q2_K.gguf
Usage
Windows
Double-click run_Tinyllama.bat or run:

python main.py
macOS/Linux
python3 main.py
Configuration
Edit config/settings.json to customize:

model_path: Path to your GGUF model file
temperature: Response creativity (0.1-1.0)
max_tokens: Maximum response length
gpu_layers: Number of layers to run on GPU (requires GPU support)
logging_enabled: Enable/disable chat history logging
Project Structure
TinyllamaChatbot/
├── main.py                 # Application entry point
├── model/                  # Model files directory
│   └── Tinyllama-1B-instruct-v0.2.Q2_K.gguf
├── core/                   # Core functionality
│   ├── chatbot.py         # Model loading and inference
│   ├── prompt_manager.py  # Prompt formatting and history
│   └── utils.py           # Utility functions
├── gui/                    # GUI components
│   └── interface.py       # Tkinter interface
├── config/                 # Configuration files
│   └── settings.json      # Application settings
├── logs/                   # Log files
│   ├── chat_history.txt   # Chat conversations
│   └── Tinyllama_chatbot.log # Application logs
├── requirements.txt        # Python dependencies
├── README.md              # This file
└── run_Tinyllama.bat        # Windows launcher
GUI Features
Status Indicator: Shows model loading status and current state
Chat Display: Scrollable conversation history with timestamps
Input Box: Multi-line text input with Enter to send
Buttons: Send message, clear chat, and model info
Menu Bar: Export history, clear data, and help options
Keyboard Shortcuts
Enter: Send message
Shift+Enter: New line in input
Ctrl+L: Clear chat (when implemented)
Troubleshooting
Model Not Loading
Ensure the model file exists in the correct path
Check that you have enough RAM (at least 4GB free)
Verify the model file is not corrupted
Slow Performance
Reduce max_tokens in settings
Set gpu_layers to 0 if you don’t have a compatible GPU
Close other memory-intensive applications
GUI Issues
Ensure tkinter is installed (usually comes with Python)
Try running with python -m tkinter to test tkinter installation
Model Information
This chatbot uses the Tinyllama 1B Instruct v0.2 model in GGUF format:

Size: ~2.8GB (Q2_K quantization)
Context Length: 2048 tokens
Language: Primarily English
License: Apache 2.0
Logging
The application logs:

Chat conversations to logs/chat_history.txt
Application events to logs/Tinyllama_chatbot.log
Configurable via settings.json
Contributing
Feel free to submit issues and enhancement requests!

License
This project is open source. Please respect the Tinyllama model license terms.

Disclaimer
This is an offline AI chatbot for educational and personal use. The responses are generated by an AI model and may not always be accurate or appropriate.